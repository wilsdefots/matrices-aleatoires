---
title: "Matrices Aléatoires"
author: " Wilson F."
format: 
  html:
    toc: true
    toc-title: "Plan"
editor: visual
embed-resources: true
---

```{r librairies, echo=FALSE,warning=FALSE, include=FALSE}
library(tidyverse)
library(knitr)
library(tibble)

```

# Démarches réalisées pour l'obtention du projet

Dans le cadre de ma formation, j’ai manifesté mon intérêt à réaliser un projet d’été en adressant un courriel à la direction du programme. Cette initiative visait à approfondir et à mettre en application les concepts étudiés dans le cours d’Apprentissage machine, en les reliant à la théorie des matrices aléatoires, un champ d’étude mathématique riche qui offre des perspectives intéressantes pour mieux comprendre le fonctionnement interne des réseaux de neurones. Plus précisément, le projet s’intéresse à l’évolution de la distribution des valeurs propres des matrices de poids des différentes couches d’un perceptron durant son entraînement.

Ce projet me permettra également de mobiliser les notions abordées dans les cours de visualisation, d’introduction à l’intelligence d’affaires, de traitement statistique des données massives et d’outils de programmation pour la science des données.

La direction a accepté ma proposition, sous réserve que je trouve un enseignant disposé à assurer l’encadrement du projet. À cet effet, j’ai sollicité M. Aurélin Nicosia, qui a accepté d’assumer ce rôle. Par la suite, nous avons complété les documents requis et élaboré un plan d’action précisant les objectifs à atteindre ainsi que les modalités de réalisation du projet.

# Sujet : Exploration des valeurs propres matrices de poids dans les réseaux de neurones

## Contexte

Les réseaux de neurones profonds sont au cœur des avancées récentes en intelligence artificielle. Malgré leurs performances remarquables, les fondements théoriques expliquant leur efficacité demeurent partiellement compris. En particulier, la structure interne des matrices de poids et leur évolution au cours de l’apprentissage font l’objet d’un intérêt croissant dans la communauté scientifique.

C’est dans ce contexte que la théorie des matrices aléatoires (Random Matrix Theory, RMT) s’avère particulièrement pertinente ; elle fournit des outils puissants pour analyser le comportement statistique de grandes matrices, notamment à travers l’étude de leurs valeurs propres (ou des valeurs singulières). Un résultat fondamental de cette théorie est la loi de Marchenko–Pastur *\[1\]*, qui décrit la densité limite du spectre des matrices de covariance issues de matrices aléatoires à grande dimension.

Plus précisément, si X est une matrice aléatoire de taille $p \times n$ , dont les entrées sont indépendantes, identiquement distribuées (iid), d'espérance nulle et de variance finie $\sigma^2$, alors, lorsque $p,n \rightarrow \infty$, avec $\frac{n}{p} \rightarrow q \in (0,\infty)$, la matrice de covariance $Y_n = \frac{1}{n}XX^T$ a un spectre dont la distribution limite suit la loi de Marchenko–Pastur. La densité de cette loi est donnée par :

$$
f_q(x) = 
\begin{cases}
\displaystyle
\frac{1}{2\pi x q \sigma^2} \sqrt{(b - x)(x - a)} & \text{si } a \leq x \leq b \\
0 & \text{sinon}
\end{cases}
\tag{1}
$$

avec $a = \sigma^2 (1-\sqrt{q})^2$ et $b = \sigma ^2(1+\sqrt{q})^2$

## Exemple

Si *q = 0.5* et \$\\sigma\$ = 1 on a :

```{r exemple, echo=FALSE}
q=0.5
sigma = 1
a = sigma**2 *(1-sqrt(q))**2
b = sigma**2 *(1+sqrt(q))**2

```

**a = `r round(a,2)`**

**b = `r round(b,2)`**

et la fonction de densité sera donnée par : $$
f_{0.5}(x) = 
\begin{cases}
\displaystyle
\frac{1}{\pi *x} \sqrt{(b - x)(x - a)} & \text{si } a \leq x \leq b \\
0 & \text{sinon}
\end{cases}
\tag{2}
$$ et sa courbe représentative est la suivante :

```{r courbe représentative, fig.height=4.5, echo=FALSE, warning=FALSE}

f <- function(x) {
  y <- sqrt((b - x) * (x - a)) / (pi * x)
  y[x < a | x > b] <- 0  
  return(y)
}

X <- seq(0, 4, by = 0.01)
Y <- f(X)

df <- data.frame(X, Y)


ggplot(df, aes(x = X, y = Y)) +
  geom_line(color = "blue") +
  labs(title = "Densité de Marchenko–Pastur, q = 0.5",
       x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

Cette distribution peut être interprétée comme le "comportement attendu" d’une matrice de covariance purement aléatoire, sans structure ni apprentissage. Ainsi, dans le contexte des réseaux de neurones, tout écart entre le spectre empirique des matrices de poids et la loi de Marchenko–Pastur peut signaler la présence de structure apprise, d’information ou de régularité.

Plus précisément :

-   Si les valeurs singulieres des matrices de poids restent dans l'intervalle $[\sqrt{a},\sqrt{b}]$ (Ce qui équivaut au fait que les valeurs propres des matrices de covariances $Y_n$ restent dans l'intervalle \[a,b\]) on est dans le bruit. La distribution est purement aléatoire, ça signifie que la matrice ne contient aucune information, ni structure cachée.

-   Si elle s'écartent de cet intervalle, on observe du signal. On peut déduire que l'algorithme a commencé à apprendre des données, les poids ont été mis à jours en fonction, et l'aléa est perdu peu à peu

-   Des valeurs isolées (outliers) signalent de la structure apprise. la structure devient utile.

-   Cependant, si on observe des valeurs propres extrèmement éloignées du spectre, cela peu indiquer un surapprentissage

-   En somme, le spectre de Marchenko–Pastur \[a,b\] est un indicateur théorique qui nous permettra de savoir à quel point les poids sont utiles pour la compréhension des données ou aléatoires.

Ce projet s’appuie sur cette intuition pour analyser les matrices de poids d’un perceptron multicouche (MLP) entraîné sur le jeu de données Fashion-MNIST. À différentes étapes de l'entraînement. Les matrices de poids de chaque couche seront sauvegardées. Leurs valeurs singulières seront alors calculées, et leur densité spectrale empirique (Empirical Spectral Density, ESD) sera construite. Ces spectres seront ensuite comparés à la densité théorique de Marchenko–Pastur, afin de mieux comprendre dans quelle mesure les matrices de poids d’un réseau profond se structurent et s’écartent de l'aléatoire au fil de l’apprentissage.

# Objectifs

L'objectif principal de ce projet est de comprendre et analyser l'évolution de la distribution des valeurs propres des matrices de poids d'un perceptron multicouche pendant l'entrainement, et comparer cette distribution à celle prédite par la fonction de densité de Marchenko-pastur, pour les matrices aléatoires.

Pour cela nous allons :

-   Construire un perceptron multicouche et l'entrainer sur le jeu de données Fashion-MNIST, en sauvegardant les matrices de poids lors de l'entrainement ;

-   Calculer les valeurs propres/singulières de ces matrices ;

-   Comparer visuellement les distributions des matrices de poids avec la distribution théorique issue de la loi de Marchenko-pastur ;

-   Interpréter les résultats.

# Structure des matrices de poids

## Rappel important pour les calculs

Le jeu de données Fashion‑MNIST contient 70000 images en niveaux de gris représentant des articles de mode, réparties en 60000 pour l'entrainement et 10000 pour le test.\
Chaque image est de taille 28 × 28 pixels, soit 784 pixels au total. Chaque pixel prend une valeur entre 0 (noir) et 255 (blanc).

Ce jeu de données est inclus nativement dans la bibliothèque tensorflow.datasets.

L’objectif est souvent de classer les images en 10 catégories, correspondant aux types d’articles suivants :

```{r classes, echo=FALSE}
classes <- tibble(
  Classe = c("T-shirt/top", "Pantalon", "Pull", "Robe", "Manteau",
             "Sandale", "Chemise", "Soulier", "Sac", "Botte"),
  etiquette = 0:9
)

classes %>%
  t %>%
  kable()
```

```{r fashon_Mnist, echo=FALSE}
include_graphics("fig/images_mnist.png")

```

## structure interne des couches

Pour notre projet, nous utilisons un réseau de neurones entièrement connecté (MLP) composé de quatre couches linéaires (C'est à dire sans convolution).

Nous faisons les choix suivants (sans perte de généralité) :

```{r structure-des-couches, echo=FALSE}
reseau_couches <- data.frame(
  Couche = c("Couche 1 (entrée)", "Couche 2", "Couche 3", "Couche 4 (sortie)"),
  Taille_entree = c(784, 512, 256, 128),
  Taille_sortie = c(512, 256, 128, 10),
  Matrice = c("W1", "W2", "W3", "W4"),
  Dimensions  = c("512 x 784", "256 x 512", "128 x 256", "10 x 128")
)

reseau_couches%>%
  t %>%
  kable()

```

Chaque matrice de poids $W_i$  représente les connexions entre les neurones de la couche précédente et ceux de la couche suivante. Ces matrices sont au cœur de notre étude, car elles évoluent pendant l'entraînement et portent potentiellement des signatures statistiques que nous analyserons à l’aide de la théorie des matrices aléatoires.

Durant l'apprentissage, les $W_i$ se mettent à jour progressivement en fonction des résultats des algorithmes de retropropagation du gradian \[3\]. La session suivante consiste à calculer l'intervalle $[a_i,b_i]$ (le Bulk)

de l'équation (1) pour chacune des couches

```{r intervalle, echo=FALSE}
#premiere couche
q1 = 32/49
a1 = (1-sqrt(q1))**2
b1 = (1+sqrt(q1))**2

#deuxieme couche
q2 = 256/512
a2 = (1-sqrt(q2))**2
b2 = (1+sqrt(q2))**2

#troisieme couche
q3 = 128/256
a3 = (1-sqrt(q3))**2
b3 = (1+sqrt(q3))**2

#quatrieme couche
q4 = 10/128
a4 = (1-sqrt(q4))**2
b4 = (1+sqrt(q4))**2


```

-   **La première couche :**

    sachant que $dim(W_1) = 512 \times 784$,

    \[a,b\] $\approx [$ `r round(a1,2)` $\sigma^2$ , `r round(b1,2)`$\sigma^2 ]$

-   **La deuxieme couche :**

    $dim(W_2) = 256\times 512$

    \[a,b\] $\approx [$ `r round(a2,2)` $\sigma^2$ , `r round(b2,2)`$\sigma^2 ]$

-   **La troisième couche :**

    $dim(W_3) = 128\times 256$

    \[a,b\] $\approx [$ `r round(a3,2)` $\sigma^2$ , `r round(b3,2)`$\sigma^2 ]$

-   **La quatrième couche :**

    $dim(W_4) = 10\times 128$

    \[a,b\] $\approx [$ `r round(a4,2)` $\sigma^2$ , `r round(b4,2)`$\sigma^2 ]$
    
- Ci-dessous, la courbe de précision de l'entraînement du perceptron sur 15 époques.

Notons que, dans l’algorithme d’apprentissage, nous avons imposé une initialisation aléatoire des poids.
Nous avons volontairement évité d’utiliser les poids proposés par Keras, car ceux-ci sont sélectionnés après une phase d’initialisation informée, ayant déjà capté certaines régularités des données ; ils sont donc optimisés pour une convergence plus rapide.

L’objectif de cette étude n’est pas d’obtenir un modèle performant. Au contraire, nous cherchons à obtenir un modèle qui apprend le plus lentement possible, afin de mieux observer l’évolution de la distribution des valeurs propres des matrices de poids au fil de l’entraînement.

```{r courbe-precision, echo=FALSE}
include_graphics("fig/Courbe_precision.png")

```


- Remarque : Pour les simulations présentées, on a choisit  $\sigma = 0.0125$

Bien que les autres valeurs de $\sigma$ produisent des résultats comparables, celle ci dessus  a été privilégiée pour des considérations esthétiques ; notament pour améliorer la lisibilité des graphiques, et les interprétations plus évidentes.

## PMC

le notebook est disponible dans mon compte google colab, via le lien suivant :

<https://colab.research.google.com/drive/1A3jfmFXuzGMHtEUyoHNi8TFfOnPvzJPO#scrollTo=HFRsFCDYjOem>

# Resultats et conclusions :

ci dessous les résultats de notre étude

```{r resultat, echo=FALSE}
include_graphics("fig/spectres_par_epoque_et_couche.png")

```

-   Pour les couches 1 et 2, on se rend compte que, jusqu'à l'époque 3, toutes les valeurs propres sont à l'intérieur de l'intervalle \[a,b\]. À partir de la quatrième époque, elles sortent progressivement de l'intervalle, et la queue s'allonge au fur et à mesure que les époques progressent.

Ceci signifie que les poids sont aléatoires jusqu'à la troisième époque, et le modèle commence à apprendre dès la quatrième époque, et cet apprentissage se renforce au cours du temps.

-   Pour la troisième couche, dès la première époque, on a juste quelques valeurs qui sortent de l'intervalle, mais à mesure qu'on avance, ce comportement ne s'améliore pas jusqu'à la dixième époque. Ce qui laisse penser que, le fait que les valeurs sortent de l'intervalle, n'est pas dû au modèle appris (d'autant plus que les poids sont choisis initialement de façon aléatoire), mais juste au hasard.

-   Pour la quatrième couche, aucune valeur propre n'est dans l'intervalle \[a,b\] tout le long de l'expérience. Donc le modèle a appris dès le début, malgré que les poids sont pris aléatoirement.

Ce paradoxe est normal, vu que la quatrième couche est la couche de sortie, et les données du Mnist sont très faciles a apprendre par un reseau de neuronne profond. Peut être que le phénomène serait différent si on prenait des données plus bruitées.

# Références

\[1\] BAI, Zhidong et SILVERSTEIN, Jack W., *Spectral analysis of large dimensional random matrices*, 2e éd., New York, Springer, 2010. (Springer Series in Statistics)

\[2\] TAO, Terence, *Topics in Random Matrix Theory*, Providence (RI), American Mathematical Society, 2012. (Graduate Studies in Mathematics, vol. 132)

\[3\] COUILLET, Romain et LIAO, Zhenyu, *Random Matrix Methods for Machine Learning*, manuscrit non publié, 19 octobre 2022.

\[4\] Levent Sagun, Léon Bottou et Yann LeCun, *EIGENVALUES OF THE HESSIAN IN DEEP LEARNING: SINGULARITY AND BEYOND, Cornell University, 22 Nov. 2010*
